We'll start with a CrAnData object just generated by the chrom_io module. 

We'll then add our sequences to adata['sequences'], write to disk and load from disk so that they will be out of memory. 

The goal here is to fully rewrite a new CrAnDataModule and MetaCrAnDataModule, removing the dataset classes and relying on xbatcher for sampling


An simple example xbatcher for a CrAnData is as follows:

import xbatcher

dim_dict = dict(adata.dims)
batch_size = 1  # Each batch will include one "var" slice.
dim_dict['var'] = batch_size
del dim_dict['item'] #Delete any 0 dimensional dims items
bgen = xbatcher.BatchGenerator(
    ds=adata,
    input_dims=dim_dict,
    batch_dims={'var':batch_size},
)

# Test the dataloader by iterating over the batches and printing variable shapes.
for i, batch in enumerate(bgen):
    print(f"Batch {i}:")
    for var_name, da in batch.data_vars.items():
        print(f"  {var_name}: shape {da.shape}")

#you can also index in as if it were a list (so this can be given to pytorch which can random sample batches)
bgen[1]

We provide this generator to a pytorch dataloader (possibly as a datapipe, or multiple datapipes for the MetaModule), which should be able to sample from the batches probabilitically.  

We'll need to make sure that in the pytorch aggr function, the DNATransform class gets called on ['sequences']. Finally for dimensions that are supposed to be shuffled, shuffle each of them using the same order for each array.


Here is the code that we'll be replacing. Do not use unnecessary code from these modules, write a totally new Module system with simple dataloaders to replace the function of the below.



"""_anndatamodule.py – Module for wrapping Yanndata-based AnnDataset and AnnDataLoader.

This module packages the dataset, dataloader, and sampler functionality so that you can
load data from your Yanndata files.
"""

from __future__ import annotations
from os import PathLike
import numpy as np
from ._genome import Genome, _resolve_genome #Just copied from crested
from anndata import AnnData
from .crandata import CrAnData
from ._dataloader import AnnDataLoader
from ._dataset import AnnDataset, MetaAnnDataset
from collections import defaultdict

def set_stage_sample_probs(adata: CrAnData, stage: str):
    required_cols = ["split"]
    for c in required_cols:
        if c not in adata.var:
            raise KeyError(f"Missing column {c} in adata.var")
    sample_probs = np.zeros(adata.n_vars, dtype=float)
    if stage == "train":
        mask = (adata.var["split"] == "train")
        if "train_probs" not in adata.var:
            adata.var["train_probs"] = 1.0
        adata.var["train_probs"] = adata.var["train_probs"] / adata.var["train_probs"].sum()
        sample_probs[mask] = adata.var["train_probs"][mask].values
        adata.var["sample_probs"] = sample_probs / sample_probs.sum()
        mask = (adata.var["split"] == "val")
        adata.var["val_probs"] = mask.astype(float)
        adata.var["val_probs"] = adata.var["val_probs"] / adata.var["val_probs"].sum()
    elif stage == "test":
        mask = (adata.var["split"] == "test")
        adata.var["test_probs"] = mask.astype(float)
        adata.var["test_probs"] = adata.var["test_probs"] / adata.var["test_probs"].sum()
    elif stage == "predict":
        adata.var["predict_probs"] = 1.0
        adata.var["predict_probs"] = adata.var["predict_probs"] / adata.var["predict_probs"].sum()
    else:
        print("Invalid stage, sample probabilities unchanged")

class AnnDataModule:
    """
    DataModule that wraps an AnnData (CrAnData) object using AnnDataset with a unified data_sources interface.
    
    Parameters:
      adata : CrAnData
          CrAnData object containing the data.
      genome : PathLike | Genome | None
          Genome instance or FASTA path.
      chromsizes_file : PathLike | None
          Path to chromsizes file.
      in_memory : bool
          If True, load sequences into memory.
      always_reverse_complement : bool
          If True, always add reverse complement sequences.
      random_reverse_complement : bool
          If True, randomly reverse complement during training.
      max_stochastic_shift : int
          Maximum random shift.
      deterministic_shift : bool
          Use legacy shifting if True.
      shuffle : bool
          Whether to shuffle training data.
      shuffle_obs : bool
          Whether to shuffle the obs dimension of each batch
      batch_size : int
          Samples per batch.
      data_sources : dict[str, str]
          Mapping of keys to data sources.
    """
    def __init__(
        self,
        adata: CrAnData,
        genome: PathLike | Genome | None = None,
        chromsizes_file: PathLike | None = None,
        in_memory: bool = True,
        always_reverse_complement: bool = True,
        random_reverse_complement: bool = False,
        max_stochastic_shift: int = 0,
        deterministic_shift: bool = False,
        shuffle: bool = True,
        batch_size: int = 256,
        data_sources: dict[str, str] = {'y': 'X'},
    ):
        self.adata = adata
        self.genome = _resolve_genome(genome, chromsizes_file)
        self.in_memory = in_memory
        self.always_reverse_complement = always_reverse_complement
        self.random_reverse_complement = random_reverse_complement
        self.max_stochastic_shift = max_stochastic_shift
        self.deterministic_shift = deterministic_shift
        self.shuffle = shuffle
        self.shuffle_obs = shuffle_obs
        self.batch_size = batch_size
        self.data_sources = data_sources

        self.train_dataset = None
        self.val_dataset = None
        self.test_dataset = None
        self.predict_dataset = None

    @staticmethod
    def _split_anndata(adata: CrAnData, split: str) -> CrAnData:
        if split:
            if "split" not in adata.var.columns:
                raise KeyError("No split column found in adata.var. Run the appropriate pre-processing.")
        return adata

    def setup(self, stage: str) -> None:
        args = {
            "adata": self.adata,
            "genome": self.genome,
            "data_sources": self.data_sources,
            "in_memory": self.in_memory,
            "always_reverse_complement": self.always_reverse_complement,
            "random_reverse_complement": self.random_reverse_complement,
            "max_stochastic_shift": self.max_stochastic_shift,
            "deterministic_shift": self.deterministic_shift,
            "split": None,
        }
        if stage == "fit":
            train_args = args.copy()
            train_args["split"] = "train"
            set_stage_sample_probs(self.adata, "train")
            self.train_dataset = AnnDataset(**train_args)
            val_args = args.copy()
            val_args["split"] = "val"
            val_args["always_reverse_complement"] = False
            val_args["random_reverse_complement"] = False
            val_args["max_stochastic_shift"] = 0
            self.val_dataset = AnnDataset(**val_args)
        elif stage == "test":
            test_args = args.copy()
            test_args["split"] = "test"
            test_args["in_memory"] = False
            test_args["always_reverse_complement"] = False
            test_args["random_reverse_complement"] = False
            test_args["max_stochastic_shift"] = 0
            set_stage_sample_probs(self.adata, "test")
            self.test_dataset = AnnDataset(**test_args)
        elif stage == "predict":
            predict_args = args.copy()
            predict_args["split"] = None
            predict_args["in_memory"] = False
            predict_args["always_reverse_complement"] = False
            predict_args["random_reverse_complement"] = False
            predict_args["max_stochastic_shift"] = 0
            set_stage_sample_probs(self.adata, "predict")
            self.predict_dataset = AnnDataset(**predict_args)
        else:
            raise ValueError(f"Invalid stage: {stage}")

    @property
    def train_dataloader(self):
        if self.train_dataset is None:
            raise ValueError("Train dataset not set. Run setup('fit') first.")
        return AnnDataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=self.shuffle,
            shuffle_obs=self.shuffle_obs,
            drop_remainder=False,
            stage='train'
        )

    @property
    def val_dataloader(self):
        if self.val_dataset is None:
            raise ValueError("Val dataset not set. Run setup('fit') first.")
        return AnnDataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            drop_remainder=False,
            stage='val'
        )

    @property
    def test_dataloader(self):
        if self.test_dataset is None:
            raise ValueError("Test dataset not set. Run setup('test') first.")
        return AnnDataLoader(
            self.test_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            drop_remainder=False,
            stage='test'
        )

    @property
    def predict_dataloader(self):
        if self.predict_dataset is None:
            raise ValueError("Predict dataset not set. Run setup('predict') first.")
        return AnnDataLoader(
            self.predict_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            drop_remainder=False,
            stage='predict'
        )

    def __repr__(self):
        return (f"AnnDataModule(adata_shape={self.adata.shape}, genome={self.genome}, "
                f"in_memory={self.in_memory}, always_reverse_complement={self.always_reverse_complement}, "
                f"random_reverse_complement={self.random_reverse_complement}, max_stochastic_shift={self.max_stochastic_shift}, "
                f"shuffle={self.shuffle}, batch_size={self.batch_size})")


class MetaAnnDataModule:
    """
    DataModule for combining multiple AnnData objects (e.g. one per species)
    into a single MetaAnnDataset for global weighted sampling.

    Each AnnData (ideally a CrAnData) is first wrapped in an AnnDataset, and then the 
    resulting datasets are merged into a MetaAnnDataset.

    Parameters
    ----------
    adatas : list[CrAnData]
        Each species/dataset stored in its own CrAnData.
    genomes : list[Genome]
        Matching list of genome references.
    data_sources : dict[str, str], default {'y': 'X'}
        Mapping of keys to data sources.
    in_memory : bool, default True
        Whether to load sequences into memory.
    random_reverse_complement : bool, default True
        Whether to randomly reverse complement each region.
    max_stochastic_shift : int, default 0
        Maximum shift (±bp) for augmentation.
    deterministic_shift : bool, default False
        If True, apply legacy fixed-stride shifting.
    shuffle : bool, default True
        Whether to shuffle the dataset.
    batch_size : int, default 256
        Number of samples per batch.
    epoch_size : int, default 100_000
        Number of samples per epoch for custom sampling.
    """
    def __init__(
        self,
        adatas: list[CrAnData],
        genomes: list[Genome],
        data_sources: dict[str, str] = {'y': 'X'},
        in_memory: bool = True,
        random_reverse_complement: bool = True,
        max_stochastic_shift: int = 0,
        deterministic_shift: bool = False,
        shuffle: bool = True,
        shuffle_obs: bool = False,
        batch_size: int = 256,
        epoch_size: int = 100_000,
        obs_alignment: str = 'union',
    ):
        if len(adatas) != len(genomes):
            raise ValueError("Must provide as many adatas as genomes.")
        
        self.adatas = adatas
        self.genomes = genomes
        self.in_memory = in_memory
        self.random_reverse_complement = random_reverse_complement
        self.max_stochastic_shift = max_stochastic_shift
        self.deterministic_shift = deterministic_shift
        self.shuffle = shuffle
        self.batch_size = batch_size
        self.data_sources = data_sources
        self.epoch_size = epoch_size
        self.shuffle_obs = shuffle_obs

        self.train_dataset = None
        self.val_dataset = None
        self.test_dataset = None
        self.predict_dataset = None
        
        # Compute observation names alignment across adatas:
        if obs_alignment == 'union':
            meta_obs_names = np.array(sorted(set().union(*[set(adata.obs_names) for adata in self.adatas])))
        elif obs_alignment == 'intersect':
            meta_obs_names = np.array(sorted(set.intersection(*[set(adata.obs_names) for adata in self.adatas])))
        else:
            raise ValueError("obs_alignment must be 'union' or 'intersect'")
        self.meta_obs_names = meta_obs_names
        for adata in self.adatas:
            adata.meta_obs_names = self.meta_obs_names

    def setup(self, stage: str) -> None:
        def dataset_args(split):
            return {
                "in_memory": self.in_memory,
                "data_sources": self.data_sources,
                "always_reverse_complement": False,  # Disable augmentation by default in meta mode
                "random_reverse_complement": self.random_reverse_complement,
                "max_stochastic_shift": self.max_stochastic_shift,
                "deterministic_shift": self.deterministic_shift,
                "split": split,
            }
        if stage == "fit":
            train_datasets = []
            val_datasets = []
            for adata, genome in zip(self.adatas, self.genomes):
                args = dataset_args("train")
                set_stage_sample_probs(adata, "train")
                ds_train = AnnDataset(adata=adata, genome=genome, **args)
                train_datasets.append(ds_train)

                val_args = dataset_args("val")
                val_args["always_reverse_complement"] = False
                val_args["random_reverse_complement"] = False
                val_args["max_stochastic_shift"] = 0
                ds_val = AnnDataset(adata=adata, genome=genome, **val_args)
                val_datasets.append(ds_val)
            self.train_dataset = MetaAnnDataset(train_datasets)
            self.val_dataset = MetaAnnDataset(val_datasets)
            
            # for ds in self.train_dataset.datasets:  
            #     # Assume each dataset is an AnnDataset that wraps a CrAnData with adata.var and augmented_probs.
            #     # Group local (augmented) indices by their chunk_index (which was added to adata.var)
            #     chunk_groups = defaultdict(list)
            #     for local_idx, chunk in enumerate(ds.adata.var["chunk_index"].to_numpy()):
            #         # Here you may need to adjust if the augmented indices differ from the order in adata.var.
            #         chunk_groups[chunk].append(local_idx)
            #     ds.chunk_groups = dict(chunk_groups)
            #     # Compute the sum of the unnormalized augmented probabilities within each chunk:
            #     chunk_weights = {}
            #     for ch, indices in ds.chunk_groups.items():
            #         # ds.augmented_probs should be a numpy array whose length matches the number of variables.
            #         chunk_weights[ch] = ds.augmented_probs[indices].sum()
            #     ds.chunk_weights = chunk_weights

            for ds in self.train_dataset.datasets:
                if "chunk_index" in ds.adata.var.columns:
                    chunk_groups = defaultdict(list)
                    for local_idx, chunk in enumerate(ds.adata.var["chunk_index"].to_numpy()):
                        chunk_groups[chunk].append(local_idx)
                else:
                    # If missing, assume all variables belong to one chunk.
                    chunk_groups = {0: list(range(len(ds.adata.var)))}
                ds.chunk_groups = dict(chunk_groups)
                ds.chunk_weights = {ch: ds.augmented_probs[indices].sum()
                                    for ch, indices in ds.chunk_groups.items()}
            
            # Also compute a per-dataset (file) weight:
            file_weights = []
            for ds in self.train_dataset.datasets:
                file_weights.append(ds.augmented_probs.sum())
            self.file_weights = np.array(file_weights)
            # Normalize to get file probabilities:
            self.file_probs = self.file_weights / self.file_weights.sum()
            self.train_dataset.file_probs = self.file_probs

        elif stage == "test":
            test_datasets = []
            for adata, genome in zip(self.adatas, self.genomes):
                args = dataset_args("test")
                set_stage_sample_probs(adata, "test")
                args["in_memory"] = False
                args["always_reverse_complement"] = False
                args["random_reverse_complement"] = False
                args["max_stochastic_shift"] = 0
                ds_test = AnnDataset(adata=adata, genome=genome, **args)
                test_datasets.append(ds_test)
            self.test_dataset = MetaAnnDataset(test_datasets)

        elif stage == "predict":
            predict_datasets = []
            for adata, genome in zip(self.adatas, self.genomes):
                args = dataset_args(None)
                set_stage_sample_probs(adata, "predict")
                args["in_memory"] = False
                args["always_reverse_complement"] = False
                args["random_reverse_complement"] = False
                args["max_stochastic_shift"] = 0
                ds_pred = AnnDataset(adata=adata, genome=genome, **args)
                predict_datasets.append(ds_pred)
            self.predict_dataset = MetaAnnDataset(predict_datasets)

        else:
            raise ValueError(f"Invalid stage: {stage}")


    @property
    def train_dataloader(self):
        if self.train_dataset is None:
            raise ValueError("train_dataset is not set. Run setup('fit') first.")
        return AnnDataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=self.shuffle,
            shuffle_obs=self.shuffle_obs,
            drop_remainder=False,
            epoch_size=self.epoch_size,
            stage='train'
        )

    @property
    def val_dataloader(self):
        if self.val_dataset is None:
            raise ValueError("val_dataset is not set. Run setup('fit') first.")
        return AnnDataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            drop_remainder=False,
            epoch_size=self.epoch_size,
            stage='val'
        )

    @property
    def test_dataloader(self):
        if self.test_dataset is None:
            raise ValueError("test_dataset is not set. Run setup('test') first.")
        return AnnDataLoader(
            self.test_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            drop_remainder=False,
            epoch_size=self.epoch_size,
            stage='test'
        )

    @property
    def predict_dataloader(self):
        if self.predict_dataset is None:
            raise ValueError("predict_dataset is not set. Run setup('predict') first.")
        return AnnDataLoader(
            self.predict_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            drop_remainder=False,
            epoch_size=self.epoch_size,
            stage='predict'
        )

    def __repr__(self):
        return (
            f"MetaAnnDataModule(num_species={len(self.adatas)}, batch_size={self.batch_size}, "
            f"shuffle={self.shuffle}, max_stochastic_shift={self.max_stochastic_shift}, "
            f"random_reverse_complement={self.random_reverse_complement}, "
            f"in_memory={self.in_memory}, "
            f"deterministic_shift={self.deterministic_shift}, epoch_size={self.epoch_size})"
        )



"""_dataloader.py – Dataloader for batching, shuffling, and one-hot encoding of CrAnData-based AnnDataset objects."""

from __future__ import annotations
import os
from collections import defaultdict
import numpy as np
import torch
from torch.utils.data import DataLoader, Sampler
import xarray as xr
from .crandata import LazyData
from .crandata import reindex_obs_array
import h5py

try:
    import sparse
except ImportError:
    sparse = None

# Import the updated AnnDataset and MetaAnnDataset that now wrap a CrAnData object.
from ._dataset import AnnDataset, MetaAnnDataset

os.environ["KERAS_BACKEND"] = "torch" #TODO TF stuff, nmj
# if os.environ.get("KERAS_BACKEND", "") == "torch":
#     import torch
#     from torch.utils.data import DataLoader, Sampler
# else:
#     import tensorflow as tf

def _shuffle_obs_in_sample(sample: dict) -> dict:
    """
    Given a sample dictionary (mapping keys to NumPy arrays),
    determine a permutation from 0 to N-1 based on one key (here, we assume the
    observation dimension is the first dimension and that at least one key has that dimension).
    Then, for every key whose first dimension matches that length, apply the permutation.
    """
    # Here we assume that the key "sequence" exists and its first dimension is N.
    # You may adjust this if your sample dictionaries use another key.
    if "sequence" in sample:
        n_obs = sample["sequence"].shape[0]
    else:
        # Alternatively, if no "sequence" key is present, use the first key that has ndim>=1.
        n_obs = None
        for val in sample.values():
            arr = np.asarray(val)
            if arr.ndim > 0:
                n_obs = arr.shape[0]
                break
    if n_obs is None:
        return sample
    perm = np.random.permutation(n_obs)
    new_sample = {}
    for key, val in sample.items():
        arr = np.asarray(val)
        if arr.ndim > 0 and arr.shape[0] == n_obs:
            new_sample[key] = arr[perm]
        else:
            new_sample[key] = arr
    return new_sample

class WeightedRegionSampler(Sampler):
    """
    Sampler that randomly samples augmented region indices from a CrAnData-based AnnDataset
    in proportion to their (nonzero) weights (augmented_probs). Used for training.
    """
    def __init__(self, dataset: AnnDataset, epoch_size: int = 100_000):
        super().__init__(data_source=dataset)
        self.dataset = dataset
        self.epoch_size = epoch_size
        p = dataset.augmented_probs
        s = p.sum()
        if s <= 0:
            raise ValueError("All sample probabilities are zero, cannot sample.")
        self.probs = p / s

    def __iter__(self):
        n = len(self.dataset.index_manager.augmented_indices)
        for _ in range(self.epoch_size):
            yield np.random.choice(n, p=self.probs)

    def __len__(self):
        return self.epoch_size


class NonShuffleRegionSampler(Sampler):
    """
    Sampler that deterministically iterates over all augmented region indices (with nonzero probability)
    exactly once in ascending order. Typically used for validation or test stages.
    """
    def __init__(self, dataset: AnnDataset):
        super().__init__(data_source=dataset)
        self.dataset = dataset
        # Filter out indices with zero probability.
        p = self.dataset.augmented_probs
        self.nonzero_indices = np.flatnonzero(p > 0.0)
        if len(self.nonzero_indices) == 0:
            raise ValueError("No nonzero probabilities for val/test stage.")

    def __iter__(self):
        return iter(self.nonzero_indices)

    def __len__(self):
        return len(self.nonzero_indices)


class MetaSampler(Sampler):
    """
    Sampler for a MetaAnnDataset that yields global indices according to global_probs.
    Used primarily during training to sample across multiple CrAnData-based AnnDataset objects.
    """
    def __init__(self, meta_dataset: MetaAnnDataset, epoch_size: int = 100_000):
        super().__init__(data_source=meta_dataset)
        self.meta_dataset = meta_dataset
        self.epoch_size = epoch_size
        s = self.meta_dataset.global_probs.sum()
        if not np.isclose(s, 1.0, atol=1e-6):
            raise ValueError(
                "Global probabilities do not sum to 1 after normalization. Sum = {}".format(s)
            )

    def __iter__(self):
        n = len(self.meta_dataset)
        p = self.meta_dataset.global_probs
        for _ in range(self.epoch_size):
            yield np.random.choice(n, p=p)

    def __len__(self):
        return self.epoch_size


class NonShuffleMetaSampler(Sampler):
    """
    Sampler for MetaAnnDataset that enumerates all global indices (with nonzero probability)
    exactly once in a deterministic order. Typically used for validation or testing.
    """
    def __init__(self, meta_dataset: MetaAnnDataset, sort: bool = True):
        super().__init__(data_source=meta_dataset)
        self.meta_dataset = meta_dataset
        p = self.meta_dataset.global_probs
        self.nonzero_global_indices = np.flatnonzero(p > 0)
        if sort:
            self.nonzero_global_indices.sort()

    def __iter__(self):
        return iter(self.nonzero_global_indices)

    def __len__(self):
        return len(self.nonzero_global_indices)

class GroupedChunkMetaSampler(Sampler):
    """
    Sampler for a MetaAnnDataset that, for each sample in an epoch, first chooses one dataset
    (i.e. one file) with probability proportional to its total unnormalized probability,
    then selects one chunk within that file with probability proportional to the sum of probabilities
    in that chunk, and finally samples a local index within that chunk according to its probability.
    
    This approach ensures that in one batch you can load from a single file and a single chunk,
    reducing I/O overhead while still preserving some mixing across conditions.
    """
    def __init__(self, meta_dataset: MetaAnnDataset, epoch_size: int = 100_000):
        super().__init__(data_source=meta_dataset)
        self.meta_dataset = meta_dataset
        self.epoch_size = epoch_size
        # meta_dataset.file_probs should have been computed as shown above.
        self.file_probs = meta_dataset.file_probs

    def __iter__(self):
        for _ in range(self.epoch_size):
            # Sample one dataset (file) based on its total probability
            ds_idx = np.random.choice(len(self.meta_dataset.datasets), p=self.file_probs)
            dataset = self.meta_dataset.datasets[ds_idx]
            
            # Now get the list of chunks available in this dataset and compute chunk probabilities
            chunk_keys = list(dataset.chunk_weights.keys())
            chunk_weights = np.array([dataset.chunk_weights[ch] for ch in chunk_keys])
            if chunk_weights.sum() <= 0:
                # If by chance no probability remains, sample uniformly from chunks
                chunk_probs = np.ones_like(chunk_weights) / len(chunk_weights)
            else:
                chunk_probs = chunk_weights / chunk_weights.sum()
            
            # Sample one chunk from the chosen dataset
            chosen_chunk = np.random.choice(chunk_keys, p=chunk_probs)
            
            # Within the chosen chunk, get the local indices and their associated probabilities
            local_indices = dataset.chunk_groups[chosen_chunk]
            local_probs = dataset.augmented_probs[local_indices]
            if local_probs.sum() <= 0:
                local_probs = np.ones_like(local_probs)
            else:
                local_probs = local_probs / local_probs.sum()
            chosen_local_idx = np.random.choice(local_indices, p=local_probs)
            
            # Yield the global index as a tuple (dataset index, local index)
            yield (ds_idx, chosen_local_idx)

    def __len__(self):
        return self.epoch_size


class AnnDataLoader:
    """
    Pytorch-like DataLoader for CrAnData-based AnnDataset (or MetaAnnDataset) objects.
    Provides batching, shuffling, and one-hot encoding for genomic sequences and additional data.

    Parameters
    ----------
    dataset
        An instance of AnnDataset or MetaAnnDataset that wraps a CrAnData object.
    batch_size
        Number of samples per batch.
    shuffle
        Whether to shuffle the dataset.
    drop_remainder
        If True, drops the last incomplete batch.
    epoch_size
        Number of samples to draw in one epoch (for weighted sampling).
    stage
        Stage indicator ("train", "val", "test", etc.) used to select the appropriate sampler.

    Example
    -------
    >>> dataset = AnnDataset(...)  # CrAnData-based dataset instance
    >>> loader = AnnDataLoader(dataset, batch_size=32, shuffle=True, stage="train")
    >>> for batch in loader.data:
    ...     # Process the batch
    """
    def __init__(
        self,
        dataset,  # can be AnnDataset or MetaAnnDataset
        batch_size: int,
        shuffle: bool = False,
        drop_remainder: bool = True,
        epoch_size: int = 100_000,
        stage: str = "train",
        shuffle_obs: bool = False,
    ):
        self.dataset = dataset
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.drop_remainder = drop_remainder
        self.epoch_size = epoch_size
        self.stage = stage
        self.shuffle_obs = shuffle_obs

        if os.environ.get("KERAS_BACKEND", "") == "torch":
            import torch
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = None

        self.sampler = None

        # If the dataset is a MetaAnnDataset, use meta-level sampling
        if isinstance(dataset, MetaAnnDataset):
            if self.stage == "train":
                # self.sampler = MetaSampler(dataset, epoch_size=self.epoch_size)
                self.sampler = GroupedChunkMetaSampler(dataset, epoch_size=self.epoch_size)
                
            else:
                self.sampler = NonShuffleMetaSampler(dataset, sort=True)
        else:
            # For a single AnnDataset with augmented probabilities
            if getattr(dataset, "augmented_probs", None) is not None:
                if self.stage == "train":
                    self.sampler = WeightedRegionSampler(dataset, epoch_size=self.epoch_size)
                else:
                    self.sampler = NonShuffleRegionSampler(dataset)
            else:
                # Fallback: use uniform shuffling if available
                if self.shuffle and hasattr(self.dataset, "shuffle"):
                    self.dataset.shuffle = True

    def batch_collate_fn(self, batch):
        """
        Collate function that groups LazyData objects by their underlying lazy_obj so that 
        the file is opened only once per group. For non-lazy items, it explicitly densifies 
        sparse arrays and reindexes them (padding missing obs with NaN) so that all arrays 
        have the same observation dimension. Finally, a single permutation is applied to the 
        obs dimension across all keys.
        """
        collated = {}
        
        # If we're dealing with a MetaAnnDataset, use the meta_obs_names from the first dataset.
        global_obs = None
        local_obs = None
        if isinstance(self.dataset, MetaAnnDataset) and len(self.dataset.datasets) > 0:
            global_obs = np.array(self.dataset.datasets[0].adata.meta_obs_names)
            local_obs = np.array(self.dataset.datasets[0].adata.obs.index)
        
        for key in batch[0]:
            # If every sample's value for this key is a LazyData instance, process them in groups.
            if all(isinstance(sample[key], LazyData) for sample in batch):
                groups = {}
                for i, sample in enumerate(batch):
                    ld = sample[key]
                    group_id = id(ld.lazy_obj)
                    groups.setdefault(group_id, []).append((i, ld.key, ld))
                results = [None] * len(batch)
                for group in groups.values():
                    lazy_obj = group[0][2].lazy_obj
                    keys = [item[1] for item in group]
                    with h5py.File(lazy_obj.filename, "r") as f:
                        dset = f[lazy_obj.dataset_name]
                        group_data = [dset[k] for k in keys]
                    for j, (i, _, ld) in enumerate(group):
                        data_item = group_data[j]
                        # Reindex if necessary.
                        if ld.global_obs is not None and data_item.shape[0] < len(ld.global_obs) and key not in ["sequence"]:
                            data_item = reindex_obs_array(data_item, ld.local_obs, ld.global_obs)
                        results[i] = data_item
                collated_value = torch.stack([torch.as_tensor(r, dtype=torch.float32) for r in results], dim=1)
            else:
                # Standard collation for non-lazy items.
                tensors = []
                for sample in batch:
                    val = sample[key]
                    # If the value supports densification (e.g. a sparse array wrapped in an xarray), do so.
                    if hasattr(val, "data") and hasattr(val.data, "todense"):
                        arr = np.array(val.data.todense())
                    else:
                        arr = np.array(val)
                    if arr.ndim == 0:
                        arr = np.expand_dims(arr, 0)
                    # If we have global_obs and the obs dimension is not the expected length, reindex.
                    if global_obs is not None and key not in ["sequence"] and arr.shape[0] != len(global_obs):
                        arr = reindex_obs_array(arr, local_obs, global_obs)
                    tensors.append(torch.as_tensor(arr, dtype=torch.float32))
                collated_value = torch.stack(tensors, dim=1)
            collated[key] = collated_value
    
        # Now compute a single permutation for the obs dimension.
        if self.shuffle_obs:
            # Determine the expected observation size from global_obs if available.
            obs_dim = None
            if global_obs is not None:
                obs_dim = len(global_obs)
            else:
                # Otherwise, use the first dimension of one key.
                for tensor in collated.values():
                    if tensor.ndim > 0:
                        obs_dim = tensor.shape[0]
                        break
            if obs_dim is not None:
                perm = torch.randperm(obs_dim)
                # Apply the same permutation to every key whose first dimension equals obs_dim.
                for key in collated:
                    print('batch len',len(batch))
                    print(key,collated[key].shape)
                    if collated[key].ndim > 0 and collated[key].shape[0] == obs_dim:
                        collated[key] = collated[key][perm]
        if self.device is not None:
            for key in collated:
                collated[key] = collated[key].to(self.device)
                if collated[key].shape[1] == len(batch) and collated[key].shape[0] != len(batch):
                    # print('I permute',collated[key].shape)
                    collated[key] = collated[key].permute((1, 0) + tuple(range(2, collated[key].ndim)))
                    # print('new shape',collated[key].shape)
        return collated
    
    def _create_dataset(self):
        from torch.utils.data import DataLoader
        if os.environ.get("KERAS_BACKEND", "") == "torch":
            if self.sampler is not None:
                return DataLoader(
                    self.dataset,
                    batch_size=self.batch_size,
                    sampler=self.sampler,
                    drop_last=self.drop_remainder,
                    num_workers=0,
                    collate_fn=self.batch_collate_fn,
                )
            else:
                return DataLoader(
                    self.dataset,
                    batch_size=self.batch_size,
                    shuffle=self.shuffle,
                    drop_last=self.drop_remainder,
                    num_workers=0,
                    collate_fn=self.batch_collate_fn,
                )
        elif os.environ.get("KERAS_BACKEND", "") == "tensorflow": #Someone who knows tf will have to deal with this
            ds = tf.data.Dataset.from_generator(
                self.dataset,
                output_signature=(
                    tf.TensorSpec(shape=(self.dataset.seq_len, 4), dtype=tf.float32),
                    tf.TensorSpec(shape=(self.dataset.num_outputs,), dtype=tf.float32),
                ),
            )
            ds = (
                ds.batch(self.batch_size, drop_remainder=self.drop_remainder)
                  .repeat()
                  .prefetch(tf.data.AUTOTUNE)
            )
            return ds

    @property
    def data(self):
        return self._create_dataset()

    def __len__(self):
        if self.sampler is not None:
            return (self.epoch_size + self.batch_size - 1) // self.batch_size
        else:
            return (len(self.dataset) + self.batch_size - 1) // self.batch_size

    def __repr__(self):
        return (
            f"AnnDataLoader(dataset={self.dataset}, batch_size={self.batch_size}, "
            f"shuffle={self.shuffle}, drop_remainder={self.drop_remainder}, shuffle_obs={self.shuffle_obs})"
        )
