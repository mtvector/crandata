{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94acffde-a523-4bc6-9fd7-d6c0487fbe02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-07 20:01:56.898\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcrandata.chrom_io\u001b[0m:\u001b[36mimport_bigwigs\u001b[0m:\u001b[36m347\u001b[0m - \u001b[1mExtracting values from 1 bigWig files...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary BED lines: ['chr1\\t100\\t200\\tchr1:100-200\\t100\\t200\\n', 'chr1\\t300\\t400\\tchr1:300-400\\t300\\t400\\n', 'chr1\\t350\\t450\\tchr1:350-450\\t350\\t450\\n']\n",
      "Extracted values shapes: [(100,), (100,), (100,)]\n",
      "Temporary BED lines: ['chr1\\t100\\t200\\tchr1:100-200\\t100\\t200\\n', 'chr1\\t300\\t400\\tchr1:300-400\\t300\\t400\\n', 'chr1\\t350\\t450\\tchr1:350-450\\t350\\t450\\n']\n",
      "Extracted values shapes: [(100,), (100,), (100,)]\n",
      "Wrote row 1/1 from /scratch/fast/45820/tmpbx4dlr6q/bigwigs/test.bw\n",
      "Added Hi-C contact data to adata.varp['hic_contacts']:\n",
      "Shape: (3, 3, 1)\n",
      "<xarray.DataArray (var_0: 3, var_1: 3, hic_file: 1)> Size: 140B\n",
      "<COO: shape=(3, 3, 1), dtype=float32, nnz=5, fill_value=0.0>\n",
      "Coordinates:\n",
      "  * var_0     (var_0) object 24B 'chr1:100-200' 'chr1:300-400' 'chr1:350-450'\n",
      "  * var_1     (var_1) object 24B 'chr1:100-200' 'chr1:300-400' 'chr1:350-450'\n",
      "  * hic_file  (hic_file) int64 8B 0\n",
      "\n",
      "Directory contents: ['beds', 'bigwigs', 'chrom.sizes', 'consensus.bed', 'chrom_data.h5', 'synthetic.bedp', 'adata.h5']\n",
      "\n",
      "Loaded CrAnData from HDF5:\n",
      "CrAnData object with 1 observations and 3 variables\n",
      "obs:\n",
      "                                            file_path\n",
      "test  /scratch/fast/45820/tmpbx4dlr6q/bigwigs/test.bw\n",
      "var:\n",
      "             chrom  start  end   chr\n",
      "region                              \n",
      "chr1:100-200  chr1    100  200  chr1\n",
      "chr1:300-400  chr1    300  400  chr1\n",
      "chr1:350-450  chr1    350  450  chr1\n",
      "varp keys: ['hic_contacts']\n",
      "Hi-C contact data shape: (3, 3, 1)\n",
      "<xarray.DataArray (var_0: 3, var_1: 3, hic_file: 1)> Size: 140B\n",
      "<COO: shape=(3, 3, 1), dtype=float32, nnz=5, fill_value=0.0>\n",
      "Coordinates:\n",
      "  * var_0     (var_0) object 24B 'chr1:100-200' 'chr1:300-400' 'chr1:350-450'\n",
      "  * var_1     (var_1) object 24B 'chr1:100-200' 'chr1:300-400' 'chr1:350-450'\n",
      "  * hic_file  (hic_file) int64 8B 0\n",
      "2025-03-07T20:02:06.384089-0800 INFO Loading sequences into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 20197.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-07T20:02:06.401042-0800 INFO Loading sequences into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 17573.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-07T20:02:06.418742-0800 INFO Loading sequences into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 18669.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-07T20:02:06.432898-0800 INFO Loading sequences into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 17697.49it/s]\n",
      "/home/matthew.schmitz/Matthew/utils/miniforge3/envs/crested/lib/python3.12/site-packages/torch/utils/data/sampler.py:76: UserWarning: `data_source` argument is not used and will be removed in 2.2.0.You may still have custom implementation that utilizes it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iterating over a couple of training batches from MetaAnnDataModule:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:02<00:00,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta Batch 0:\n",
      "  sequence: shape torch.Size([100, 3, 4])\n",
      "  y: shape torch.Size([1, 3, 100])\n",
      "  hic: shape torch.Size([3, 3, 1])\n",
      "  gex: shape torch.Size([1, 3, 100])\n",
      "Meta Batch 1:\n",
      "  sequence: shape torch.Size([100, 3, 4])\n",
      "  y: shape torch.Size([1, 3, 100])\n",
      "  hic: shape torch.Size([3, 3, 1])\n",
      "  gex: shape torch.Size([1, 3, 100])\n",
      "Meta Batch 2:\n",
      "  sequence: shape torch.Size([100, 3, 4])\n",
      "  y: shape torch.Size([1, 3, 100])\n",
      "  hic: shape torch.Size([3, 3, 1])\n",
      "  gex: shape torch.Size([1, 3, 100])\n",
      "Meta Batch 3:\n",
      "  sequence: shape torch.Size([100, 1, 4])\n",
      "  y: shape torch.Size([1, 1, 100])\n",
      "  hic: shape torch.Size([3, 1, 1])\n",
      "  gex: shape torch.Size([1, 1, 100])\n",
      "['beds', 'bigwigs', 'chrom.sizes', 'consensus.bed', 'chrom_data.h5', 'synthetic.bedp', 'adata.h5', 'chr1.fa', 'chr1.fa.fai']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyBigWig\n",
    "import importlib\n",
    "import xarray as xr\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import crandata\n",
    "crandata = importlib.reload(crandata)\n",
    "\n",
    "\n",
    "import crandata.yanndata\n",
    "import crandata.chrom_io\n",
    "crandata.yanndata = importlib.reload(crandata.yanndata)\n",
    "crandata.chrom_io = importlib.reload(crandata.chrom_io)\n",
    "crandata._anndatamodule = importlib.reload(crandata._anndatamodule)\n",
    "# crandata._dataloader = importlib.reload(crandata._dataloader)\n",
    "# crandata._dataset = importlib.reload(crandata._dataset)\n",
    "crandata._anndatamodule = importlib.reload(crandata._anndatamodule)\n",
    "from crandata._anndatamodule import MetaAnnDataModule\n",
    "\n",
    "# Create temporary directories for beds, bigwigs, etc.\n",
    "temp_dir = tempfile.TemporaryDirectory()\n",
    "base_dir = Path(temp_dir.name)\n",
    "beds_dir = base_dir / \"beds\"\n",
    "bigwigs_dir = base_dir / \"bigwigs\"\n",
    "beds_dir.mkdir(exist_ok=True)\n",
    "bigwigs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create a chromsizes file\n",
    "chromsizes_file = base_dir / \"chrom.sizes\"\n",
    "with open(chromsizes_file, \"w\") as f:\n",
    "    f.write(\"chr1\\t1000\\n\")\n",
    "\n",
    "# Create two BED files (ClassA and ClassB)\n",
    "bed_data_A = pd.DataFrame({0: [\"chr1\", \"chr1\"],\n",
    "                           1: [100, 300],\n",
    "                           2: [200, 400]})\n",
    "bed_data_B = pd.DataFrame({0: [\"chr1\", \"chr1\"],\n",
    "                           1: [150, 350],\n",
    "                           2: [250, 450]})\n",
    "bed_file_A = beds_dir / \"ClassA.bed\"\n",
    "bed_file_B = beds_dir / \"ClassB.bed\"\n",
    "bed_data_A.to_csv(bed_file_A, sep=\"\\t\", header=False, index=False)\n",
    "bed_data_B.to_csv(bed_file_B, sep=\"\\t\", header=False, index=False)\n",
    "\n",
    "# Create a consensus BED file\n",
    "consensus = pd.DataFrame({0: [\"chr1\", \"chr1\", \"chr1\"],\n",
    "                          1: [100, 300, 350],\n",
    "                          2: [200, 400, 450]})\n",
    "consensus_file = base_dir / \"consensus.bed\"\n",
    "consensus.to_csv(consensus_file, sep=\"\\t\", header=False, index=False)\n",
    "\n",
    "# Create a bigWig file with a single chromosome region\n",
    "bigwig_file = bigwigs_dir / \"test.bw\"\n",
    "bw = pyBigWig.open(str(bigwig_file), \"w\")\n",
    "bw.addHeader([(\"chr1\", 1000)])\n",
    "bw.addEntries(chroms=[\"chr1\"], starts=[0], ends=[1000], values=[5.0])\n",
    "bw.close()\n",
    "\n",
    "# Set parameters for extraction\n",
    "target_region_width = 100\n",
    "backed_path = os.path.join(base_dir, \"chrom_data.h5\")\n",
    "\n",
    "# Create the CrAnData object from the bigWig files and consensus regions\n",
    "adata = crandata.chrom_io.import_bigwigs(\n",
    "    bigwigs_folder=bigwigs_dir,\n",
    "    regions_file=consensus_file,\n",
    "    backed_path=backed_path,\n",
    "    target_region_width=target_region_width,\n",
    "    chromsizes_file=chromsizes_file,\n",
    "    \n",
    ")\n",
    "\n",
    "# Add a random obsm entry\n",
    "adata.obsm['gex'] = xr.DataArray(np.random.randn(adata.obs.shape[0], 100),\n",
    "                                 dims=['types', 'genes'])\n",
    "\n",
    "# Create a synthetic BEDP file for Hi-C contacts and add contacts to adata.varp\n",
    "synthetic_bedp = pd.DataFrame({\n",
    "    0: [\"chr1\", \"chr1\"],\n",
    "    1: [100, 300],\n",
    "    2: [200, 400],\n",
    "    3: [\"chr1\", \"chr1\"],\n",
    "    4: [150, 350],\n",
    "    5: [250, 450],\n",
    "    6: [10, 20]\n",
    "})\n",
    "synthetic_bedp_file = base_dir / \"synthetic.bedp\"\n",
    "synthetic_bedp.to_csv(synthetic_bedp_file, sep=\"\\t\", header=False, index=False)\n",
    "\n",
    "from crandata.chrom_io import add_contact_strengths_to_varp\n",
    "contacts = add_contact_strengths_to_varp(adata, [str(synthetic_bedp_file)], key=\"hic_contacts\")\n",
    "\n",
    "print(\"Added Hi-C contact data to adata.varp['hic_contacts']:\")\n",
    "print(\"Shape:\", adata.varp[\"hic_contacts\"].shape)\n",
    "print(adata.varp[\"hic_contacts\"])\n",
    "\n",
    "\n",
    "# Write to HDF5 and load back.\n",
    "h5_path = os.path.join(base_dir, \"adata.h5\")\n",
    "adata.to_h5(h5_path)\n",
    "adata_loaded = crandata.yanndata.CrAnData.from_h5(h5_path,backed=['X'])\n",
    "print(\"\\nDirectory contents:\", os.listdir(base_dir))\n",
    "print(\"\\nLoaded CrAnData from HDF5:\")\n",
    "print(adata_loaded)\n",
    "print(\"obs:\")\n",
    "print(adata_loaded.obs)\n",
    "print(\"var:\")\n",
    "print(adata_loaded.var)\n",
    "print(\"varp keys:\", list(adata_loaded.varp.keys()))\n",
    "if \"hic_contacts\" in adata_loaded.varp:\n",
    "    print(\"Hi-C contact data shape:\", adata_loaded.varp[\"hic_contacts\"].shape)\n",
    "    print(adata_loaded.varp[\"hic_contacts\"])\n",
    "\n",
    "# ----- Extended test: Create multiple Yanndata-based modules and a MetaAnnDataModule, then test dataloading -----\n",
    "\n",
    "# Create two copies of the loaded CrAnData (simulate two different datasets/species)\n",
    "adata1 = adata_loaded.copy()\n",
    "adata2 = adata_loaded.copy()\n",
    "# Ensure each has a 'split' column\n",
    "adata1.var[\"split\"] = \"train\"\n",
    "adata2.var[\"split\"] = \"train\"\n",
    "\n",
    "# Create a dummy FASTA file for the genome (with a single record for chr1)\n",
    "fasta_file = base_dir / \"chr1.fa\"\n",
    "with open(fasta_file, \"w\") as f:\n",
    "    f.write(\">chr1\\n\")\n",
    "    f.write(\"A\" * 1000 + \"\\n\")\n",
    "\n",
    "# Instead of passing a string, create a Genome object.\n",
    "from crandata._genome import Genome\n",
    "dummy_genome = Genome(str(fasta_file), chrom_sizes=str(chromsizes_file))\n",
    "\n",
    "# Import MetaAnnDataModule (using the package name so that relative imports resolve)\n",
    "\n",
    "\n",
    "# Instantiate MetaAnnDataModule with the two datasets and corresponding genomes.\n",
    "meta_module = MetaAnnDataModule(\n",
    "    adatas=[adata1, adata2],\n",
    "    genomes=[dummy_genome, dummy_genome],\n",
    "    data_sources={'y': 'X','hic':'varp/hic_contacts','gex':'obsm/gex'},\n",
    "    in_memory=True,\n",
    "    random_reverse_complement=True,\n",
    "    max_stochastic_shift=5,\n",
    "    deterministic_shift=False,\n",
    "    shuffle_obs=True,\n",
    "    shuffle=True,\n",
    "    batch_size=3,    # small batch size for testing\n",
    "    epoch_size=10    # small epoch size for quick testing\n",
    ")\n",
    "\n",
    "# Setup the meta module for the \"fit\" stage (train/val)\n",
    "meta_module.setup(\"fit\")\n",
    "\n",
    "# Retrieve the training dataloader from the meta module and iterate over a couple of batches.\n",
    "meta_train_dl = meta_module.train_dataloader\n",
    "\n",
    "print(\"\\nIterating over a couple of training batches from MetaAnnDataModule:\")\n",
    "for i, batch in enumerate(tqdm.tqdm(meta_train_dl.data)):\n",
    "    print(f\"Meta Batch {i}:\")\n",
    "    for key, tensor in batch.items():\n",
    "        print(f\"  {key}: shape {tensor.shape}\")\n",
    "    # if i == 1:\n",
    "    #     break\n",
    "\n",
    "print(os.listdir(base_dir))\n",
    "\n",
    "temp_dir.cleanup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70a77bd5-624b-4ad9-b34f-873368677945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test    /scratch/fast/45820/tmpbx4dlr6q/bigwigs/test.bw\n",
       "Name: file_path, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obs['file_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "381dcf6e-5799-4d53-bef5-ed27fce29dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test    /scratch/fast/45820/tmpbx4dlr6q/bigwigs/test.bw\n",
       "Name: file_path, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_loaded.obs['file_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7648c91a-2516-48c8-b33c-c629a55a9d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crandata.yanndata._XWrapper"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(adata_loaded.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27b42817-ca8f-44c4-aead-599cdf8b8d43",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adata_loaded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43madata_loaded\u001b[49m\u001b[38;5;241m.\u001b[39mX\n",
      "\u001b[0;31mNameError\u001b[0m: name 'adata_loaded' is not defined"
     ]
    }
   ],
   "source": [
    "adata_loaded.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133aed5e-5c51-4d4c-9b2f-18813365c89d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crested",
   "language": "python",
   "name": "crested"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
