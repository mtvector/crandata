{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94acffde-a523-4bc6-9fd7-d6c0487fbe02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/matthew.schmitz/Matthew/utils/miniforge3/envs/crested/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/scratch/fast/46103/ipykernel_3995050/2205277207.py\", line 15, in <module>\n",
      "    import crandata.yanndata\n",
      "ModuleNotFoundError: No module named 'crandata.yanndata'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/matthew.schmitz/Matthew/utils/miniforge3/envs/crested/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 2168, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/matthew.schmitz/Matthew/utils/miniforge3/envs/crested/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1457, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/matthew.schmitz/Matthew/utils/miniforge3/envs/crested/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1348, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/matthew.schmitz/Matthew/utils/miniforge3/envs/crested/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1195, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/matthew.schmitz/Matthew/utils/miniforge3/envs/crested/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1085, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/matthew.schmitz/Matthew/utils/miniforge3/envs/crested/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1153, in get_records\n",
      "    mod = inspect.getmodule(cf.tb_frame)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/matthew.schmitz/Matthew/utils/miniforge3/envs/crested/lib/python3.12/inspect.py\", line 1013, in getmodule\n",
      "    f = getabsfile(module)\n",
      "        ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/matthew.schmitz/Matthew/utils/miniforge3/envs/crested/lib/python3.12/inspect.py\", line 982, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/matthew.schmitz/Matthew/utils/miniforge3/envs/crested/lib/python3.12/inspect.py\", line 955, in getsourcefile\n",
      "    filename = getfile(object)\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"/home/matthew.schmitz/Matthew/utils/miniforge3/envs/crested/lib/python3.12/site-packages/torch/package/package_importer.py\", line 730, in _patched_getfile\n",
      "    return _orig_getfile(object)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/matthew.schmitz/Matthew/utils/miniforge3/envs/crested/lib/python3.12/inspect.py\", line 916, in getfile\n",
      "    raise TypeError('{!r} is a built-in module'.format(object))\n",
      "TypeError: <module 'crandata' (namespace) from ['/allen/programs/celltypes/workgroups/rnaseqanalysis/EvoGen/Team/Matthew/code/crandata/tests/../../crandata']> is a built-in module\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyBigWig\n",
    "import importlib\n",
    "import xarray as xr\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import crandata\n",
    "crandata = importlib.reload(crandata)\n",
    "\n",
    "\n",
    "import crandata.yanndata\n",
    "import crandata.chrom_io\n",
    "crandata.yanndata = importlib.reload(crandata.yanndata)\n",
    "crandata.chrom_io = importlib.reload(crandata.chrom_io)\n",
    "crandata._anndatamodule = importlib.reload(crandata._anndatamodule)\n",
    "# crandata._dataloader = importlib.reload(crandata._dataloader)\n",
    "# crandata._dataset = importlib.reload(crandata._dataset)\n",
    "crandata._anndatamodule = importlib.reload(crandata._anndatamodule)\n",
    "from crandata._anndatamodule import MetaAnnDataModule\n",
    "\n",
    "# Create temporary directories for beds, bigwigs, etc.\n",
    "temp_dir = tempfile.TemporaryDirectory()\n",
    "base_dir = Path(temp_dir.name)\n",
    "beds_dir = base_dir / \"beds\"\n",
    "bigwigs_dir = base_dir / \"bigwigs\"\n",
    "beds_dir.mkdir(exist_ok=True)\n",
    "bigwigs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create a chromsizes file\n",
    "chromsizes_file = base_dir / \"chrom.sizes\"\n",
    "with open(chromsizes_file, \"w\") as f:\n",
    "    f.write(\"chr1\\t1000\\n\")\n",
    "\n",
    "# Create two BED files (ClassA and ClassB)\n",
    "bed_data_A = pd.DataFrame({0: [\"chr1\", \"chr1\"],\n",
    "                           1: [100, 300],\n",
    "                           2: [200, 400]})\n",
    "bed_data_B = pd.DataFrame({0: [\"chr1\", \"chr1\"],\n",
    "                           1: [150, 350],\n",
    "                           2: [250, 450]})\n",
    "bed_file_A = beds_dir / \"ClassA.bed\"\n",
    "bed_file_B = beds_dir / \"ClassB.bed\"\n",
    "bed_data_A.to_csv(bed_file_A, sep=\"\\t\", header=False, index=False)\n",
    "bed_data_B.to_csv(bed_file_B, sep=\"\\t\", header=False, index=False)\n",
    "\n",
    "# Create a consensus BED file\n",
    "consensus = pd.DataFrame({0: [\"chr1\", \"chr1\", \"chr1\"],\n",
    "                          1: [100, 300, 350],\n",
    "                          2: [200, 400, 450]})\n",
    "consensus_file = base_dir / \"consensus.bed\"\n",
    "consensus.to_csv(consensus_file, sep=\"\\t\", header=False, index=False)\n",
    "\n",
    "# Create a bigWig file with a single chromosome region\n",
    "bigwig_file = bigwigs_dir / \"test.bw\"\n",
    "bw = pyBigWig.open(str(bigwig_file), \"w\")\n",
    "bw.addHeader([(\"chr1\", 1000)])\n",
    "bw.addEntries(chroms=[\"chr1\"], starts=[0], ends=[1000], values=[5.0])\n",
    "bw.close()\n",
    "\n",
    "# Set parameters for extraction\n",
    "target_region_width = 100\n",
    "backed_path = os.path.join(base_dir, \"chrom_data.h5\")\n",
    "\n",
    "# Create the CrAnData object from the bigWig files and consensus regions\n",
    "adata = crandata.chrom_io.import_bigwigs(\n",
    "    bigwigs_folder=bigwigs_dir,\n",
    "    regions_file=consensus_file,\n",
    "    backed_path=backed_path,\n",
    "    target_region_width=target_region_width,\n",
    "    chromsizes_file=chromsizes_file,\n",
    "    \n",
    ")\n",
    "\n",
    "# Add a random obsm entry\n",
    "adata.obsm['gex'] = xr.DataArray(np.random.randn(adata.obs.shape[0], 100),\n",
    "                                 dims=['types', 'genes'])\n",
    "\n",
    "# Create a synthetic BEDP file for Hi-C contacts and add contacts to adata.varp\n",
    "synthetic_bedp = pd.DataFrame({\n",
    "    0: [\"chr1\", \"chr1\"],\n",
    "    1: [100, 300],\n",
    "    2: [200, 400],\n",
    "    3: [\"chr1\", \"chr1\"],\n",
    "    4: [150, 350],\n",
    "    5: [250, 450],\n",
    "    6: [10, 20]\n",
    "})\n",
    "synthetic_bedp_file = base_dir / \"synthetic.bedp\"\n",
    "synthetic_bedp.to_csv(synthetic_bedp_file, sep=\"\\t\", header=False, index=False)\n",
    "\n",
    "from crandata.chrom_io import add_contact_strengths_to_varp\n",
    "contacts = add_contact_strengths_to_varp(adata, [str(synthetic_bedp_file)], key=\"hic_contacts\")\n",
    "\n",
    "print(\"Added Hi-C contact data to adata.varp['hic_contacts']:\")\n",
    "print(\"Shape:\", adata.varp[\"hic_contacts\"].shape)\n",
    "print(adata.varp[\"hic_contacts\"])\n",
    "\n",
    "\n",
    "# Write to HDF5 and load back.\n",
    "h5_path = os.path.join(base_dir, \"adata.h5\")\n",
    "adata.to_h5(h5_path)\n",
    "adata_loaded = crandata.yanndata.CrAnData.from_h5(h5_path,backed=['X'])\n",
    "print(\"\\nDirectory contents:\", os.listdir(base_dir))\n",
    "print(\"\\nLoaded CrAnData from HDF5:\")\n",
    "print(adata_loaded)\n",
    "print(\"obs:\")\n",
    "print(adata_loaded.obs)\n",
    "print(\"var:\")\n",
    "print(adata_loaded.var)\n",
    "print(\"varp keys:\", list(adata_loaded.varp.keys()))\n",
    "if \"hic_contacts\" in adata_loaded.varp:\n",
    "    print(\"Hi-C contact data shape:\", adata_loaded.varp[\"hic_contacts\"].shape)\n",
    "    print(adata_loaded.varp[\"hic_contacts\"])\n",
    "\n",
    "# ----- Extended test: Create multiple Yanndata-based modules and a MetaAnnDataModule, then test dataloading -----\n",
    "\n",
    "# Create two copies of the loaded CrAnData (simulate two different datasets/species)\n",
    "adata1 = adata_loaded.copy()\n",
    "adata2 = adata_loaded.copy()\n",
    "# Ensure each has a 'split' column\n",
    "adata1.var[\"split\"] = \"train\"\n",
    "adata2.var[\"split\"] = \"train\"\n",
    "\n",
    "# Create a dummy FASTA file for the genome (with a single record for chr1)\n",
    "fasta_file = base_dir / \"chr1.fa\"\n",
    "with open(fasta_file, \"w\") as f:\n",
    "    f.write(\">chr1\\n\")\n",
    "    f.write(\"A\" * 1000 + \"\\n\")\n",
    "\n",
    "# Instead of passing a string, create a Genome object.\n",
    "from crandata._genome import Genome\n",
    "dummy_genome = Genome(str(fasta_file), chrom_sizes=str(chromsizes_file))\n",
    "\n",
    "# Import MetaAnnDataModule (using the package name so that relative imports resolve)\n",
    "\n",
    "\n",
    "# Instantiate MetaAnnDataModule with the two datasets and corresponding genomes.\n",
    "meta_module = MetaAnnDataModule(\n",
    "    adatas=[adata1, adata2],\n",
    "    genomes=[dummy_genome, dummy_genome],\n",
    "    data_sources={'y': 'X','hic':'varp/hic_contacts','gex':'obsm/gex'},\n",
    "    in_memory=True,\n",
    "    random_reverse_complement=True,\n",
    "    max_stochastic_shift=5,\n",
    "    deterministic_shift=False,\n",
    "    shuffle_obs=True,\n",
    "    shuffle=True,\n",
    "    batch_size=3,    # small batch size for testing\n",
    "    epoch_size=10    # small epoch size for quick testing\n",
    ")\n",
    "\n",
    "# Setup the meta module for the \"fit\" stage (train/val)\n",
    "meta_module.setup(\"fit\")\n",
    "\n",
    "# Retrieve the training dataloader from the meta module and iterate over a couple of batches.\n",
    "meta_train_dl = meta_module.train_dataloader\n",
    "\n",
    "print(\"\\nIterating over a couple of training batches from MetaAnnDataModule:\")\n",
    "for i, batch in enumerate(tqdm.tqdm(meta_train_dl.data)):\n",
    "    print(f\"Meta Batch {i}:\")\n",
    "    for key, tensor in batch.items():\n",
    "        print(f\"  {key}: shape {tensor.shape}\")\n",
    "    # if i == 1:\n",
    "    #     break\n",
    "\n",
    "print(os.listdir(base_dir))\n",
    "\n",
    "temp_dir.cleanup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42235987-9521-409e-86dc-a505111d1409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import crandata\n",
    "import importlib\n",
    "crandata = importlib.reload(crandata)\n",
    "crandata.chrom_io = importlib.reload(crandata.chrom_io)\n",
    "crandata.crandata = importlib.reload(crandata.crandata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "133aed5e-5c51-4d4c-9b2f-18813365c89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import crandata\n",
    "import os\n",
    "\n",
    "genome_path = '/allen/programs/celltypes/workgroups/rnaseqanalysis/EvoGen/Team/Matthew/genome/onehots/mouse'\n",
    "fasta_file = os.path.join(genome_path,'mouse.fa')\n",
    "chrom_sizes = os.path.join(genome_path,'mouse.fa.sizes')\n",
    "annotation_gtf_file = os.path.join(genome_path,'mouse.annotation.gtf')\n",
    "\n",
    "genome = crandata.Genome(fasta_file, chrom_sizes, annotation_gtf_file)\n",
    "\n",
    "# Set parameters for binning.\n",
    "WINDOW_SIZE = 2114\n",
    "OFFSET = WINDOW_SIZE // 2  # e.g., 50% overlap\n",
    "N_THRESHOLD = 0.3\n",
    "\n",
    "# Optionally specify an output path for the BED file.\n",
    "OUTPUT_BED = os.path.join(genome_path, \"binned_genome.bed\")\n",
    "\n",
    "# Generate bins and optionally write to disk.\n",
    "binned_df = crandata.bin_genome(genome, WINDOW_SIZE, OFFSET, n_threshold=N_THRESHOLD, output_path=OUTPUT_BED)\n",
    "\n",
    "print(\"Filtered bins:\")\n",
    "print(binned_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fbd99d-12d5-43c0-a1c4-96bfe68a92c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:00<00:00, 716.03it/s]\n",
      "\u001b[32m2025-03-08 23:20:33.435\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcrandata.chrom_io\u001b[0m:\u001b[36mimport_bigwigs\u001b[0m:\u001b[36m417\u001b[0m - \u001b[1mExtracting values from 49 bigWig files...\u001b[0m\n",
      "32it [31:27, 58.93s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "bigwigs_dir = '/allen/programs/celltypes/workgroups/rnaseqanalysis/EvoGen/SpinalCord/manuscript/ATAC/mouse/Group_bigwig/'\n",
    "n_bins = WINDOW_SIZE//50\n",
    "\n",
    "adata = crandata.chrom_io.import_bigwigs(\n",
    "    bigwigs_folder=bigwigs_dir,\n",
    "    regions_file=OUTPUT_BED,\n",
    "    backed_path='/home/matthew.schmitz/Matthew/mouse_spc_test.h5',\n",
    "    target_region_width=WINDOW_SIZE,\n",
    "    chromsizes_file=chrom_sizes,\n",
    "    n_bins=n_bins   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c999f883-8f01-4ddb-8f93-2d85691f4f8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43madata\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'adata' is not defined"
     ]
    }
   ],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907d3827-0f78-4a6d-8e05-0a212bb1ff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.X[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f42942-cc49-4e05-8f0c-ae283d396106",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33821977-ebe5-45fd-877a-81cba6793751",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "217d9613-3da5-40ef-8e2a-449843d4cb92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adata = crandata.crandata.CrAnData.from_h5('/home/matthew.schmitz/Matthew/mouse_spc_test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5616f7de-34a3-48bc-9ba3-a1cb5817fd2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d7baae-48c8-4c5a-a82a-5a1fb7b54064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40515d7-6174-415d-82cc-a8784249510e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f70292f-f088-4e9d-a127-4736bb591964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import crandata\n",
    "import os\n",
    "import crested\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b454e2de-689f-432e-9621-4d88f73e3b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "genomes = {}\n",
    "beds = {}\n",
    "chromsizes_files = {}\n",
    "bed_files = {}\n",
    "species = ['mouse','human','macaque']\n",
    "\n",
    "WINDOW_SIZE = 2114\n",
    "OFFSET = WINDOW_SIZE // 2  # e.g., 50% overlap\n",
    "N_THRESHOLD = 0.3\n",
    "n_bins = WINDOW_SIZE//50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392bdeca-411d-47ea-a846-6c906784b46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in species:\n",
    "    genome_path = '/allen/programs/celltypes/workgroups/rnaseqanalysis/EvoGen/Team/Matthew/genome/onehots/'+s\n",
    "    fasta_file = os.path.join(genome_path,s+'.fa')\n",
    "    chrom_sizes = os.path.join(genome_path,s+'.fa.sizes')\n",
    "    annotation_gtf_file = os.path.join(genome_path,s+'.annotation.gtf')\n",
    "    chromsizes_files[s] = chrom_sizes\n",
    "    genome = crandata.Genome(fasta_file, chrom_sizes, annotation_gtf_file)\n",
    "    genomes[s] = genome\n",
    "    # Set parameters for binning.\n",
    "    \n",
    "    # Optionally specify an output path for the BED file.\n",
    "    OUTPUT_BED = os.path.join(genome_path, \"binned_genome.bed\")\n",
    "    bed_files[s] = OUTPUT_BED\n",
    "    # Generate bins and optionally write to disk.\n",
    "    # binned_df = crandata.bin_genome(genome, WINDOW_SIZE, OFFSET, n_threshold=N_THRESHOLD, output_path=OUTPUT_BED)\n",
    "    # print(\"Filtered bins:\")\n",
    "    # print(binned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10492d8e-e22d-48a3-8139-6d5aadc02d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "adatas = {}\n",
    "\n",
    "for s in species:\n",
    "    bigwigs_dir = os.path.join('/allen/programs/celltypes/workgroups/rnaseqanalysis/EvoGen/SpinalCord/manuscript/ATAC',s,'Group_bigwig')\n",
    "    # adata = crandata.chrom_io.import_bigwigs(\n",
    "    #     bigwigs_folder=bigwigs_dir,\n",
    "    #     regions_file=bed_files[s],\n",
    "    #     backed_path='/home/matthew.schmitz/Matthew/'+s+'_spc_test.h5',\n",
    "    #     target_region_width=WINDOW_SIZE,\n",
    "    #     chromsizes_file=chromsizes_files[s],\n",
    "    #     n_bins=n_bins   \n",
    "    # )\n",
    "    # adatas[s] = adata\n",
    "    adatas[s] = crandata.crandata.CrAnData.from_h5('/home/matthew.schmitz/Matthew/'+s+'_spc_test.h5')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963ce450-9ef0-4d38-92cc-59521099497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "adatas['mouse'].uns['chunk_size'] = 512\n",
    "adatas['human'].uns['chunk_size'] = 512\n",
    "adatas['macaque'].uns['chunk_size'] = 512\n",
    "adatas['mouse'].var[\"chunk_index\"] = np.arange(adatas['mouse'].var.shape[0]) // 512\n",
    "adatas['human'].var[\"chunk_index\"] = np.arange(adatas['human'].var.shape[0]) // 512\n",
    "adatas['macaque'].var[\"chunk_index\"] = np.arange(adatas['macaque'].var.shape[0]) // 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b667671b-3508-4daf-a706-92651c186854",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in adatas.keys():\n",
    "    crested.pp.train_val_test_split(\n",
    "        adatas[s], strategy=\"region\", val_size=0.1, test_size=0.1, random_state=42\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d8de975-ee1e-49ca-af1f-eb43d3b688a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MetaAnnDataModule' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m crandata\u001b[38;5;241m.\u001b[39m_dataloader \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mreload(crandata\u001b[38;5;241m.\u001b[39m_dataloader)\n\u001b[1;32m      7\u001b[0m crandata\u001b[38;5;241m.\u001b[39m_dataset \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mreload(crandata\u001b[38;5;241m.\u001b[39m_dataset)\n\u001b[0;32m----> 9\u001b[0m MetaAnnDataModule \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mreload(\u001b[43mMetaAnnDataModule\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MetaAnnDataModule' is not defined"
     ]
    }
   ],
   "source": [
    "import crandata\n",
    "import importlib\n",
    "importlib.reload(crandata)\n",
    "importlib.reload(crandata.crandata)\n",
    "importlib.reload(crandata._anndatamodule)\n",
    "importlib.reload(crandata._dataloader)\n",
    "importlib.reload(crandata._dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aaf99e-b31e-4f93-a15d-e6ca8aa65780",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_module = crandata._anndatamodule.MetaAnnDataModule(\n",
    "    adatas=list(adatas.values()),\n",
    "    genomes=list(genomes.values()),\n",
    "    data_sources={'y': 'X'},\n",
    "    in_memory=False,\n",
    "    random_reverse_complement=True,\n",
    "    max_stochastic_shift=10,\n",
    "    deterministic_shift=False,\n",
    "    shuffle_obs=False,\n",
    "    shuffle=True,\n",
    "    batch_size=32,    # small batch siqrze for testing\n",
    "    epoch_size=1000000    # small epoch size for quick testing\n",
    ")\n",
    "\n",
    "# Setup the meta module for the \"fit\" stage (train/val)\n",
    "meta_module.setup(\"fit\")\n",
    "\n",
    "# Retrieve the training dataloader from the meta module and iterate over a couple of batches.\n",
    "meta_train_dl = meta_module.train_dataloader\n",
    "\n",
    "print(\"\\nIterating over a couple of training batches from MetaAnnDataModule:\")\n",
    "for i, batch in enumerate(tqdm(meta_train_dl.data)):\n",
    "    print(f\"Meta Batch {i}:\")\n",
    "    for key, tensor in batch.items():\n",
    "        print(f\"  {key}: shape {tensor.shape}\")\n",
    "    if i == 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca45b303-d162-4c7d-a162-95867e42c25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdeb6199-c97a-4af0-bc00-0a5292139f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthew.schmitz/Matthew/utils/miniforge3/envs/crested/lib/python3.12/site-packages/torch/utils/data/sampler.py:76: UserWarning: `data_source` argument is not used and will be removed in 2.2.0.You may still have custom implementation that utilizes it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iterating over training batches with timing instrumentation:\n",
      "AnnDataset.__getitem__ for index 424667 took 0.0355 seconds\n",
      "AnnDataset.__getitem__ for index 270952 took 0.0246 seconds\n",
      "AnnDataset.__getitem__ for index 2680552 took 0.0159 seconds\n",
      "AnnDataset.__getitem__ for index 743535 took 0.0179 seconds\n",
      "AnnDataset.__getitem__ for index 87648 took 0.0514 seconds\n",
      "AnnDataset.__getitem__ for index 211748 took 0.0152 seconds\n",
      "AnnDataset.__getitem__ for index 1684820 took 0.0207 seconds\n",
      "AnnDataset.__getitem__ for index 885137 took 0.0151 seconds\n",
      "AnnDataset.__getitem__ for index 1467636 took 0.0189 seconds\n",
      "AnnDataset.__getitem__ for index 373764 took 0.0160 seconds\n",
      "AnnDataset.__getitem__ for index 2649283 took 0.0121 seconds\n",
      "AnnDataset.__getitem__ for index 1231476 took 0.0222 seconds\n",
      "AnnDataset.__getitem__ for index 715932 took 0.0109 seconds\n",
      "AnnDataset.__getitem__ for index 783216 took 0.0178 seconds\n",
      "AnnDataset.__getitem__ for index 2250074 took 0.0124 seconds\n",
      "AnnDataset.__getitem__ for index 899124 took 0.0157 seconds\n",
      "AnnDataset.__getitem__ for index 888126 took 0.0143 seconds\n",
      "AnnDataset.__getitem__ for index 1452785 took 0.0217 seconds\n",
      "AnnDataset.__getitem__ for index 507379 took 0.0174 seconds\n",
      "AnnDataset.__getitem__ for index 2169908 took 0.0136 seconds\n",
      "AnnDataset.__getitem__ for index 362320 took 0.0007 seconds\n",
      "AnnDataset.__getitem__ for index 2101122 took 0.0136 seconds\n",
      "AnnDataset.__getitem__ for index 1500140 took 0.0171 seconds\n",
      "AnnDataset.__getitem__ for index 1533193 took 0.0219 seconds\n",
      "AnnDataset.__getitem__ for index 404623 took 0.0310 seconds\n",
      "AnnDataset.__getitem__ for index 1695624 took 0.0143 seconds\n",
      "AnnDataset.__getitem__ for index 127828 took 0.0119 seconds\n",
      "AnnDataset.__getitem__ for index 480037 took 0.0179 seconds\n",
      "AnnDataset.__getitem__ for index 2493479 took 0.0139 seconds\n",
      "AnnDataset.__getitem__ for index 2486345 took 0.0153 seconds\n",
      "AnnDataset.__getitem__ for index 422477 took 0.0007 seconds\n",
      "AnnDataset.__getitem__ for index 1589418 took 0.0110 seconds\n",
      "Collate function took 0.0012 seconds for batch of 32 samples\n",
      "\n",
      "Batch 0:\n",
      "  sequence: shape torch.Size([2114, 32, 4])\n",
      "  y: shape torch.Size([51, 32, 42])\n",
      "AnnDataset.__getitem__ for index 2033818 took 0.0333 seconds\n",
      "AnnDataset.__getitem__ for index 2326159 took 0.0202 seconds\n",
      "AnnDataset.__getitem__ for index 1490093 took 0.0169 seconds\n",
      "AnnDataset.__getitem__ for index 422783 took 0.0130 seconds\n",
      "AnnDataset.__getitem__ for index 2193408 took 0.0238 seconds\n",
      "AnnDataset.__getitem__ for index 797432 took 0.0140 seconds\n",
      "AnnDataset.__getitem__ for index 830944 took 0.0127 seconds\n",
      "AnnDataset.__getitem__ for index 1024757 took 0.0153 seconds\n",
      "AnnDataset.__getitem__ for index 2131750 took 0.0402 seconds\n",
      "AnnDataset.__getitem__ for index 2095949 took 0.0161 seconds\n",
      "AnnDataset.__getitem__ for index 2133524 took 0.0263 seconds\n",
      "AnnDataset.__getitem__ for index 170986 took 0.0188 seconds\n",
      "AnnDataset.__getitem__ for index 707833 took 0.0252 seconds\n",
      "AnnDataset.__getitem__ for index 2322765 took 0.0228 seconds\n",
      "AnnDataset.__getitem__ for index 1015741 took 0.0173 seconds\n",
      "AnnDataset.__getitem__ for index 418727 took 0.0007 seconds\n",
      "AnnDataset.__getitem__ for index 134577 took 0.0006 seconds\n",
      "AnnDataset.__getitem__ for index 213695 took 0.0007 seconds\n",
      "AnnDataset.__getitem__ for index 2213761 took 0.0168 seconds\n",
      "AnnDataset.__getitem__ for index 1643795 took 0.0207 seconds\n",
      "AnnDataset.__getitem__ for index 2190584 took 0.0161 seconds\n",
      "AnnDataset.__getitem__ for index 901709 took 0.0151 seconds\n",
      "AnnDataset.__getitem__ for index 1924367 took 0.0188 seconds\n",
      "AnnDataset.__getitem__ for index 1266921 took 0.0165 seconds\n",
      "AnnDataset.__getitem__ for index 2031428 took 0.0187 seconds\n",
      "AnnDataset.__getitem__ for index 236937 took 0.0125 seconds\n",
      "AnnDataset.__getitem__ for index 1139874 took 0.0184 seconds\n",
      "AnnDataset.__getitem__ for index 667508 took 0.0202 seconds\n",
      "AnnDataset.__getitem__ for index 178004 took 0.0118 seconds\n",
      "AnnDataset.__getitem__ for index 1531034 took 0.0091 seconds\n",
      "AnnDataset.__getitem__ for index 108315 took 0.0007 seconds\n",
      "AnnDataset.__getitem__ for index 400871 took 0.0188 seconds\n",
      "Collate function took 0.0012 seconds for batch of 32 samples\n",
      "\n",
      "Batch 1:\n",
      "  sequence: shape torch.Size([2114, 32, 4])\n",
      "  y: shape torch.Size([51, 32, 42])\n",
      "AnnDataset.__getitem__ for index 1396209 took 0.0129 seconds\n",
      "AnnDataset.__getitem__ for index 1818037 took 0.0187 seconds\n",
      "AnnDataset.__getitem__ for index 2176037 took 0.0181 seconds\n",
      "AnnDataset.__getitem__ for index 95703 took 0.0007 seconds\n",
      "AnnDataset.__getitem__ for index 348213 took 0.0151 seconds\n",
      "AnnDataset.__getitem__ for index 1039797 took 0.0135 seconds\n",
      "AnnDataset.__getitem__ for index 2588812 took 0.0144 seconds\n",
      "AnnDataset.__getitem__ for index 1831273 took 0.0125 seconds\n",
      "AnnDataset.__getitem__ for index 2659180 took 0.0328 seconds\n",
      "AnnDataset.__getitem__ for index 1632277 took 0.0139 seconds\n",
      "AnnDataset.__getitem__ for index 1726580 took 0.0161 seconds\n",
      "AnnDataset.__getitem__ for index 2155379 took 0.0142 seconds\n",
      "AnnDataset.__getitem__ for index 2427328 took 0.0150 seconds\n",
      "AnnDataset.__getitem__ for index 732006 took 0.0006 seconds\n",
      "AnnDataset.__getitem__ for index 1377951 took 0.0169 seconds\n",
      "AnnDataset.__getitem__ for index 2560259 took 0.0185 seconds\n",
      "AnnDataset.__getitem__ for index 966573 took 0.0166 seconds\n",
      "AnnDataset.__getitem__ for index 218909 took 0.0006 seconds\n",
      "AnnDataset.__getitem__ for index 2077595 took 0.0099 seconds\n",
      "AnnDataset.__getitem__ for index 2633010 took 0.0247 seconds\n",
      "AnnDataset.__getitem__ for index 1021603 took 0.0241 seconds\n",
      "AnnDataset.__getitem__ for index 1826703 took 0.0117 seconds\n",
      "AnnDataset.__getitem__ for index 2533233 took 0.0212 seconds\n",
      "AnnDataset.__getitem__ for index 1342638 took 0.0184 seconds\n",
      "AnnDataset.__getitem__ for index 126528 took 0.0007 seconds\n",
      "AnnDataset.__getitem__ for index 965504 took 0.0200 seconds\n",
      "AnnDataset.__getitem__ for index 2252706 took 0.0121 seconds\n",
      "AnnDataset.__getitem__ for index 1454684 took 0.0350 seconds\n",
      "AnnDataset.__getitem__ for index 1515756 took 0.0122 seconds\n",
      "AnnDataset.__getitem__ for index 647906 took 0.0181 seconds\n",
      "AnnDataset.__getitem__ for index 1162123 took 0.0126 seconds\n",
      "AnnDataset.__getitem__ for index 2366450 took 0.0193 seconds\n",
      "Collate function took 0.0013 seconds for batch of 32 samples\n",
      "\n",
      "Batch 2:\n",
      "  sequence: shape torch.Size([2114, 32, 4])\n",
      "  y: shape torch.Size([51, 32, 42])\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import crandata\n",
    "\n",
    "# --- Patch the AnnDataset __getitem__ method ---\n",
    "# Save the original __getitem__\n",
    "orig_getitem = crandata._dataset.AnnDataset.__getitem__\n",
    "\n",
    "def timed_getitem(self, idx):\n",
    "    start = time.perf_counter()\n",
    "    result = orig_getitem(self, idx)\n",
    "    elapsed = time.perf_counter() - start\n",
    "    print(f\"AnnDataset.__getitem__ for index {idx} took {elapsed:.4f} seconds\")\n",
    "    return result\n",
    "\n",
    "# Monkey-patch the method\n",
    "crandata._dataset.AnnDataset.__getitem__ = timed_getitem\n",
    "\n",
    "# --- Patch the collate function in AnnDataLoader ---\n",
    "# Save the original collate function (which is an instance method)\n",
    "orig_collate_fn = crandata._dataloader.AnnDataLoader._collate_fn\n",
    "\n",
    "def timed_collate_fn(self, batch):\n",
    "    start = time.perf_counter()\n",
    "    result = orig_collate_fn(self, batch)\n",
    "    elapsed = time.perf_counter() - start\n",
    "    print(f\"Collate function took {elapsed:.4f} seconds for batch of {len(batch)} samples\")\n",
    "    return result\n",
    "\n",
    "# Monkey-patch the collate function\n",
    "crandata._dataloader.AnnDataLoader._collate_fn = timed_collate_fn\n",
    "\n",
    "# --- Testing loop ---\n",
    "meta_train_dl = meta_module.train_dataloader\n",
    "\n",
    "print(\"\\nIterating over training batches with timing instrumentation:\")\n",
    "for i, batch in enumerate(meta_train_dl.data):\n",
    "    print(f\"\\nBatch {i}:\")\n",
    "    for key, tensor in batch.items():\n",
    "        print(f\"  {key}: shape {tensor.shape}\")\n",
    "    if i >= 2:  # Process a few batches for testing.\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afea2f8-171d-4478-a849-b7fab3821dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import crandata\n",
    "\n",
    "# --- Patch the LazyH5Array __getitem__ method to measure disk I/O ---\n",
    "orig_lazy_getitem = crandata.crandata.LazyH5Array.__getitem__\n",
    "\n",
    "def timed_lazy_getitem(self, key):\n",
    "    start = time.perf_counter()\n",
    "    result = orig_lazy_getitem(self, key)\n",
    "    elapsed = time.perf_counter() - start\n",
    "    print(f\"LazyH5Array.__getitem__ for key {key} took {elapsed:.4f} seconds\")\n",
    "    return result\n",
    "\n",
    "crandata.crandata.LazyH5Array.__getitem__ = timed_lazy_getitem\n",
    "\n",
    "# --- Then run your testing loop again ---\n",
    "meta_train_dl = meta_module.train_dataloader\n",
    "\n",
    "print(\"\\nIterating over training batches with extended timing instrumentation:\")\n",
    "for i, batch in enumerate(meta_train_dl.data):\n",
    "    print(f\"\\nBatch {i}:\")\n",
    "    for key, tensor in batch.items():\n",
    "        print(f\"  {key}: shape {tensor.shape}\")\n",
    "    if i >= 2:  # Process a few batches for testing.\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e661681c-6d1e-4441-9878-c21d0505df51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_architecture = crested.tl.zoo.chrombpnet(\n",
    "    seq_len=2114, num_classes=51\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6348c92d-cfce-4e39-8a7f-3df53b363b77",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create your own configuration\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# I recommend trying this for peak regression with a weighted cosine mse log loss function\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m      4\u001b[0m loss \u001b[38;5;241m=\u001b[39m crested\u001b[38;5;241m.\u001b[39mtl\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mCosineMSELogLoss(max_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m metrics \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      6\u001b[0m     keras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mMeanAbsoluteError(),\n\u001b[1;32m      7\u001b[0m     keras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mMeanSquaredError(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     crested\u001b[38;5;241m.\u001b[39mtl\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mZeroPenaltyMetric(),\n\u001b[1;32m     13\u001b[0m ]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "# Create your own configuration\n",
    "# I recommend trying this for peak regression with a weighted cosine mse log loss function\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss = crested.tl.losses.CosineMSELogLoss(max_weight=100, multiplier=1)\n",
    "metrics = [\n",
    "    keras.metrics.MeanAbsoluteError(),\n",
    "    keras.metrics.MeanSquaredError(),\n",
    "    keras.metrics.CosineSimilarity(axis=1),\n",
    "    crested.tl.metrics.PearsonCorrelation(),\n",
    "    crested.tl.metrics.ConcordanceCorrelationCoefficient(),\n",
    "    crested.tl.metrics.PearsonCorrelationLog(),\n",
    "    crested.tl.metrics.ZeroPenaltyMetric(),\n",
    "]\n",
    "\n",
    "alternative_config = crested.tl.TaskConfig(optimizer, loss, metrics)\n",
    "print(alternative_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b3716a-4a11-418e-9aad-6778019c0f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = crested.tl.Crested(\n",
    "    data=datamodule,\n",
    "    model=model_architecture,\n",
    "    config=alternative_config,\n",
    "    project_name=\"mouse_biccn\",  # change to your liking\n",
    "    run_name=\"basemodel\",  # change to your liking\n",
    "    logger=\"wandb\",  # or None, 'dvc', 'tensorboard'\n",
    "    seed=7,  # For reproducibility\n",
    ")\n",
    "# train the model\n",
    "trainer.fit(\n",
    "    epochs=60,\n",
    "    learning_rate_reduce_patience=3,\n",
    "    early_stopping_patience=6,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crested",
   "language": "python",
   "name": "crested"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
